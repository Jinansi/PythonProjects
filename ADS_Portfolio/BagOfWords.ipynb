{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Loading the data from csv file***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Replaced.csv\", encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Splitting the dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Checking the size of the dataset i.e number of rows and columns  ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56781, 22)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** First record of the \"text\" column which contains the full reviews given by a user ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love this album. it's very good. more to the hip hop side than her current pop sound.. SO HYPE! i listen to this everyday at the gym! i give it 5star rating all the way. her metaphors are just crazy.\n"
     ]
    }
   ],
   "source": [
    "print(train[\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***To extract the data from HTML into text format we use BeautifulSoup***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RIO\\AppData\\Local\\conda\\conda\\envs\\ads_project\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\RIO\\AppData\\Local\\conda\\conda\\envs\\ads_project\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "example1 = BeautifulSoup(train[\"text\"][0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Displaying the clear text of the record ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love this album. it's very good. more to the hip hop side than her current pop sound.. SO HYPE! i listen to this everyday at the gym! i give it 5star rating all the way. her metaphors are just crazy.\n"
     ]
    }
   ],
   "source": [
    "print(example1.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***In order to keep the data clean, we remove the special characters and the punctuation marks used with the help of regular expression library***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love this album  it s very good  more to the hip hop side than her current pop sound   SO HYPE  i listen to this everyday at the gym  i give it  star rating all the way  her metaphors are just crazy \n"
     ]
    }
   ],
   "source": [
    "import re   \n",
    "letters_only = re.sub(\"[^a-zA-Z]\", \" \", example1.get_text() )  \n",
    "print(letters_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Converting all the data to lowercase in order to keep a constant format***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_case = letters_only.lower()\n",
    "words = lower_case.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i love this album  it s very good  more to the hip hop side than her current pop sound   so hype  i listen to this everyday at the gym  i give it  star rating all the way  her metaphors are just crazy '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ***We use nltk for symbolic and statistical natural language processing (NLP) for English written in the Python programming language***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\RIO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk   \n",
    "nltk.download('stopwords')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** we import the stopwords in order to check the words from our \"text\" column with them ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***extracting special words (non common words) from the given row value***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'album', 'good', 'hip', 'hop', 'side', 'current', 'pop', 'sound', 'hype', 'listen', 'everyday', 'gym', 'give', 'star', 'rating', 'way', 'metaphors', 'crazy']\n"
     ]
    }
   ],
   "source": [
    "words = [w for w in words if not w in stopwords.words(\"english\")]  \n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***creating a function to clean the data and then add the non stop words to a list***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_words(raw_review):\n",
    "    review_text = BeautifulSoup(raw_review).get_text()\n",
    "    \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    \n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***for a given row, removes the stopwords and special characters and writes the sentence***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love album good hip hop side current pop sound hype listen everyday gym give star rating way metaphors crazy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RIO\\AppData\\Local\\conda\\conda\\envs\\ads_project\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\RIO\\AppData\\Local\\conda\\conda\\envs\\ads_project\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "clean_review = review_to_words(train[\"text\"][0]) \n",
    "print(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56781"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_reviews = train[\"text\"].size   #(56781)\n",
    "num_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Writing a loop to take up the first 3 records and goto the review_to_words(raw_review) function in order to extract the stopwords and display only the important terms ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love album good hip hop side current pop sound hype listen everyday gym give star rating way metaphors crazy']\n",
      "['love album good hip hop side current pop sound hype listen everyday gym give star rating way metaphors crazy', 'good flavor review collected part promotion']\n",
      "['love album good hip hop side current pop sound hype listen everyday gym give star rating way metaphors crazy', 'good flavor review collected part promotion', 'good flavor']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RIO\\AppData\\Local\\conda\\conda\\envs\\ads_project\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\RIO\\AppData\\Local\\conda\\conda\\envs\\ads_project\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "clean_train_reviews = []\n",
    "\n",
    "for i in range(0, 3):\n",
    "    clean_train_reviews.append(review_to_words(train[\"text\"][i]))\n",
    "    #len(clean_train_reviews)\n",
    "    print (clean_train_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Using countvectorizer we Convert a collection of text documents to a matrix of token counts***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* analyzer : string, {‘word’, ‘char’, ‘char_wb’} or callable\n",
    "Whether the feature should be made of word or character n-grams. Option ‘char_wb’ creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tokenizer : callable or None (default)\n",
    "Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if analyzer == 'word'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* preprocessor : callable or None (default)\n",
    "Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* stop_words : string {‘english’}, list, or None (default)\n",
    "If ‘english’, a built-in stop word list for English is used.\n",
    "If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if analyzer == 'word'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* max_features : int or None, default=None\n",
    "If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "This parameter is ignored if vocabulary is not None. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating the bag of words...\\n\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer    \n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000)\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "train_data_features = train_data_features.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 24)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Applying the vector matrix (vectorizer) and calling out the function ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['album', 'collected', 'crazy', 'current', 'everyday', 'flavor', 'give', 'good', 'gym', 'hip', 'hop', 'hype', 'listen', 'love', 'metaphors', 'part', 'pop', 'promotion', 'rating', 'review', 'side', 'sound', 'star', 'way']\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Displaying the special words and their occurences in the given dataset by summig up the count and then mapping it to the term ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 album\n",
      "1 bought\n",
      "1 boyfriend\n",
      "1 burning\n",
      "1 buying\n",
      "2 captivating\n",
      "1 caused\n",
      "1 clean\n",
      "1 collected\n",
      "1 consistency\n",
      "2 could\n",
      "1 couples\n",
      "1 crazy\n",
      "1 current\n",
      "1 difficult\n",
      "2 disappointed\n",
      "1 enhanced\n",
      "1 especially\n",
      "1 even\n",
      "1 everyday\n",
      "1 expecting\n",
      "1 felt\n",
      "2 flavor\n",
      "3 gel\n",
      "1 give\n",
      "3 good\n",
      "1 gym\n",
      "1 hip\n",
      "1 hop\n",
      "1 however\n",
      "1 husband\n",
      "1 hype\n",
      "1 irritation\n",
      "1 lacked\n",
      "2 less\n",
      "1 like\n",
      "1 liquid\n",
      "1 listen\n",
      "1 live\n",
      "1 looking\n",
      "1 love\n",
      "1 lube\n",
      "1 lubricant\n",
      "1 lubricants\n",
      "1 mess\n",
      "1 messy\n",
      "1 metaphors\n",
      "1 money\n",
      "1 much\n",
      "1 neither\n",
      "1 normal\n",
      "2 notice\n",
      "1 one\n",
      "1 paid\n",
      "1 part\n",
      "1 personal\n",
      "1 pleasant\n",
      "1 pop\n",
      "1 promotion\n",
      "1 rating\n",
      "2 read\n",
      "1 recommend\n",
      "1 reminiscent\n",
      "1 review\n",
      "2 reviews\n",
      "2 sensation\n",
      "1 side\n",
      "1 since\n",
      "1 skin\n",
      "1 sort\n",
      "1 sound\n",
      "1 star\n",
      "1 starters\n",
      "1 ultimately\n",
      "1 us\n",
      "2 use\n",
      "1 vaseline\n",
      "1 way\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print(count, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content of this project itself is licensed under the and the underlying source code used to format and display that content is licensed under the [MIT LICENSE](https://github.com/Jinansi/PythonProjects/blob/master/LICENSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
